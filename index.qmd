---
title: "Assessing Model Accuracy"
format:
  html:
    code-fold: true
    code-summary: "Show the code"
execute:
  cache: true
  echo: fenced
---

<!-- # jupyter: -->
<!-- #   kernelspec: -->
<!-- #     display_name: Gen -->
<!-- #     language: python -->
<!-- #     name: python3 -->

# Assessing Model Accuracy

## Quality of Fit

There must be some way to quantify how well our predictions match the observed data. The extent to which the predicted response value for a given observation is close to the true response for said observation. Within regressions, the most commonly used measure is the *mean squared error (MSE)*, given by $MSE = \frac{1}{n}\sum_{i = 1}^{n}(y_{i} - \hat{f}(x_{i}))^{2}$ where $\hat{f}(x_{i})$ is the prediction that $\hat{f}$ gives for the $i$th observation. The MSE will be small if the predicted responses are very close to the true response, and will be large if for some of the observations, the predicted and true responses differ substantially.

The goal, roughly speaking, is to obtain the lowest test MSE. There is no garauntee that the method with the lowest training MSE will also produce the lowest test MSE. The problem is that many statistical methods specifically estimate coefficients so as to minimize the training set MSE. The training MSE can be quite small, but the test MSE is often much larger.

```{python .code-overflow-wrap}
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import polars as pl
import seaborn as sns

pd.options.display.precision = 4

auto_data = pd.read_csv(
    "./data/ALL CSV FILES - 2nd Edition/Auto.csv", na_values=["?"]
)
autos = auto_data.dropna().reset_index().drop(columns="index")

quant_vals = [
    "mpg",
    "cylinders",
    "displacement",
    "horsepower",
    "weight",
    "acceleration",
]
ranges = [np.max(autos[column]) for column in quant_vals]
means = [np.mean(autos[column]) for column in quant_vals]
std = [np.std(autos[column]) for column in quant_vals]
maxes = [np.max(autos[column]) for column in quant_vals]
mins = [np.min(autos[column]) for column in quant_vals]

d = {
    "Max": maxes,
    "Min": mins,
    "Range": ranges,
    "Mean": means,
    "Standard Deviation": std,
}
measuredf = pd.DataFrame(d, index=quant_vals)

sns.set_theme(style="ticks")
sns.pairplot(autos)
```

::: {.callout-note}

- Observations made:
  - Number of cylinders increases, the weight increases, the rate of acceleration increases, horsepower increases, displacement increases, and efficiency (mpg) decreases.
  - As displacement increases, horsepower increases & efficiency decreases.
  - As horsepower increases, acceleration decreases, efficiency (mpg) decreases.
  - As weight increases, efficiency decreases.
  - As model year increases, efficiency increases.

:::

<!-- ::: {.panel-center}
- Observations made:
  - Number of cylinders increases, the weight increases, the rate of acceleration increases, horsepower increases, displacement  increases, and efficiency (mpg) decreases.
  - As displacement increases, horsepower increases & efficiency decreases.
  - As horsepower increases, acceleration decreases, efficiency (mpg) decreases.
  - As weight increases, efficiency decreases.
  - As model year increases, efficiency increases.
::: -->

Recall we arent interested in whether $\hat{f}(x_{o}) ≈ y_{i}$; the goal, instead, is to know whether $\hat{f}(x_{0})$ is approximately equaol to $y_{0}$, where $(x_{0}, y_{0})$ is a *previously unseen test observation not used to train the statistical learning method*. Choose the method that gives the lowest *test MSE*, as opposed to the lowest training MSE. Aka, if you have a large number of test observations, you could compute: $\text{Avg}(y_{0} - \hat{f}(x_{0}))^{2}$, the average squared prediction error for these test observations $(x_{0}, y_{0})$.

## Bias/Variance Tradeoff

### Bias

- Error between average model prediction and ground truth aka how far your average is from the truth.
- The bias of the estimated function tells us the capacity of the underlying model to predict the values
$\textit{bias} = \mathbin{E}[f\prime(x)] - f(x)$

### Variance

Formally variance, $var(x)$ -  or its square root, standard deviation - measures how densely packed around the [`Expected Value`]("./expected_value.qmd") points in the distribution are: meaning, the spread of the point distribution. Its formally defined as:
$$
var = \begin{cases}
    \sum^{n}_{i=1} \enspace(x_i - \mu)^2 ρ(x_i) &\rightarrow \text{for a discrete n point distribution} \\
    \int^{\infty}_{x=-\infty} (x - \mu)^2 ρ(x)dx &\rightarrow \text{for a continuous distribution} \\
\end{cases}
$$

- Average variability in the model prediction for the given dataset
- The variance of the estimated function tells you how much the function can adjust to the change in the dataset
$\textit{variance} = \mathbin{E}[(f^\prime(x) - \mathbin{E}[f^\prime(x)])^2]$

- High Bias
  - Overly simplified model
  - Under-fitting
  - High error on both test and train data

- High Variance
  - Overly complex model
  - Over-fitting
  - Low error on train data, high error on test data
  - Will model the noise in the input

| #   | Error Category | Bias | Variance | Train Error | Test Error |
| --- | -------------- | ---- | -------- | ----------- | ---------- |
| 1   | Overfit        | Low  | High     | Small       | Large      |
| 2   | Underfit       | High | Low      | Large       | Large      |

```{python}
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

# generate some values to showcase model complexity
model_complexity = np.linspace(0, 10, 100)

# defining functions for bias, variance and test error
bias_sq = (10 - model_complexity) ** 2 / 20
var = model_complexity**2 / 30

# test error is bias and variance together
test_error = bias_sq + var
sns.set_theme("paper")
# sns.set_palette(palette='flare')
plt.figure(figsize=(10, 6))
# plt.plot(model_complexity, bias_sq, label = r"Bias", color = "blue")
plt.plot(model_complexity, bias_sq, label="Bias")
# plt.plot(model_complexity, var, label = 'Variance', color = 'red')
plt.plot(model_complexity, var, label="Variance")
# plt.plot(model_complexity, test_error, label = 'Test Error (= Bias + Variance)', color = 'black')
plt.plot(model_complexity, test_error, label="Test Error (= Bias + Variance)")

plt.xlabel("Model Complexity", fontsize=14)
plt.ylabel("Error", fontsize=14)
plt.title("Bias-Variance Tradeoff", fontsize=16)
# plt.axvline(x = 5, color = 'gray', linestyle = '--', label = 'Optimal Trade-Off')
plt.axvline(x=5, linestyle="--", label="Optimal Trade-Off")
plt.legend()
plt.grid(True)
plt.show()
```

The bias-variance tradeoff is a fundamental concept in ML that helps us understand the tradeoff between two types of errors that a model can make: bias and variance. Its crucial to strike a balance between these two types of errors to create a model that generalizes well to new, unseen data.

**Bias** refers to the error due to overly simplistic assumptions in the learning algorithm. A high bias model may underfit the data, meaning it fails to capture the underlying patterns and relationships in the data, leading to poor performance on both the training and test sets.

**Variance** refers to the error due to the model's sensitivity to small fluctuations in the training data. A high variance model might overfit the data, meaning it fits the training data very well but fails to generalize to new data points, resulting in poor performance on the test set.

**Tradeoff Explanation with Example:**

- Consider a simple example of fitting a polynomial to a set of data points. Imagine you have a set of points that form a curve on a 2D plane. Your goal is to find a polynomial equation that fits these points:

  i. **High Bias, Low Variance:** Suppose you decide to fit a linear equation to the data points. This is a high bias model because it makes a simplistic assumption about the underlying relationship. As a result, the fitted line might not capture the curve's nuances, leading to a bias in the predictions. However, this simple model is less sensitive to variations in the training data, so it might perform similarly on both the training and test sets.

  ii. **Low Bias, High Variance:** Now consider fitting a high-degree polynomial (ex: 10th-degree polynomial) to the data points. This is a low bias model because it has the flexibility to closely follow the data points, even capturing their intricate details. However, this mode is more sensitive to the noise and fluctuations in the training data, resulting in high varince. Its likely to fit the training data extremely well but may not generalize to new data points, leading to poor performance on the test set.

In reality the tradeoff isnt so simple. You wont see such clean depictions of bias vs variance to pick an optimal point. Here is a more realistic plot:

```{python}
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures

# synthetic data
rng = np.random.default_rng(seed=42)
np.random.seed(42)
X = np.linspace(-3, 3, 1_000)
y = X**3 - 2 * X**2 + X + rng.normal(0, 3, X.shape[0])

# splitting
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)
X_train = X_train.reshape(-1, 1)
X_test = X_test.reshape(-1, 1)

# variables to store error
train_errors = []
test_errors = []
degrees = range(1, 15)

# loop over degrees to fit polynomial models of increasing complexity
for degree in degrees:
    poly_features = PolynomialFeatures(degree=degree)
    X_train_poly = poly_features.fit_transform(X_train)
    X_test_poly = poly_features.transform(X_test)

    model = LinearRegression()
    model.fit(X_train_poly, y_train)

    # calculate errors
    y_train_pred = model.predict(X_train_poly)
    y_test_pred = model.predict(X_test_poly)
    train_errors.append(mean_squared_error(y_train, y_train_pred))
    test_errors.append(mean_squared_error(y_test, y_test_pred))


# optimal degree where test error is min
optimal_degree = degrees[np.argmin(test_errors)]

# plot
plt.figure(figsize=(12, 8))
plt.plot(
    degrees,
    train_errors,
    label="Train Error",
    marker="o",
    linestyle="-",
    color="b",
)
plt.plot(
    degrees,
    test_errors,
    label="Test Error",
    marker="o",
    linestyle="-",
    color="r",
)

# sns.scatterplot(optimal_degree, min(test_errors), color = "purple", )
plt.scatter(
    optimal_degree,
    min(test_errors),
    color="purple",
    label=f"Optimal Degree ({optimal_degree = })",
    zorder=5,
)
plt.axvspan(
    degrees[0],
    optimal_degree - 1,
    color="blue",
    alpha=0.1,
    label="High Bias (Underfitting)",
)
plt.axvspan(
    optimal_degree + 1,
    degrees[-1],
    color="red",
    alpha=0.1,
    label="High Variance (Overfitting)",
)

plt.xlabel("Model Complexity (Polynomial Degree)")
plt.ylabel("Mean Squared Error")
plt.title("Bias-Variance TradeOff")
plt.legend()
plt.grid(True)
plt.show()
```

```{python}
from calendar import monthrange

import graphviz as gf
import pandas as pd
import polars as pl

# --
import tensorflow as tf
from dateutil.relativedelta import relativedelta
from sklearn import tree

# --
from sklearn.datasets import load_digits
from sklearn.ensemble import (
    AdaBoostClassifier,
    RandomForestClassifier,
    RandomForestRegressor,
)
from sklearn.linear_model import ElasticNet, Lasso, Ridge
from sklearn.metrics import RocCurveDisplay, auc, roc_curve
from sklearn.neural_network import MLPClassifier
from statsmodels.distributions.empirical_distribution import ECDF

# testing_sample = pl.from_pandas(_testing_sample)
# import pyreadr as pr
from statsmodels.formula.api import ols
from statsmodels.graphics.tsaplots import plot_acf
from tensorflow import keras
from xgboost import XGBClassifier

plt.style.use("seaborn-v0_8")
# plt.rcParams["patch.facecolor"] = "white"
plt.rcParams["figure.figsize"] = 15, 7

digits = load_digits()
X = digits.data / 16.0 - 0.5  # normalizes dataset into -0.5 to 0.5 range
y = digits.target

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.15, random_state=123, stratify=y
)

clf = MLPClassifier(
    hidden_layer_sizes=(10,),
    activation="relu",
    learning_rate_init=0.01,
    random_state=123,
    solver="adam",
    max_iter=500,
)

# Now you can convert the pandas dataframe to other formats if needed
# For example, to polars:
```

```{python}
# Normal approximation Interval based on test set
import scipy.stats as st

confidence = 0.95
z_value = st.norm.ppf((1 + confidence) / 2.0)
print(z_value)
```

----

```{python}
# RData = pr.read_r("../../Downloads/data_ml.RData")
# _data = RData["data_ml"]
# _data['date'] = pd.to_datetime(_data['date'], format = '%Y-%m-%d')
# _data.head()
```

```{python}
# _data.groupby('date')['stock_id'].count().plot(ylabel = 'n_assets', title = 'Number of distinct assets through time');
```

```{python}
# features = _data.columns.to_list()[2:-4]
# features_short = ["Div_Yld", "Eps", "Mkt_Cap_12M_Usd", "Mom_11M_Usd", "Ocf", "Pb", "Vol1Y_Usd"]

# _data.loc[(_data['date'] == '2000-2-29'), 'Div_Yld'].hist(bins = 100);
```

```{python}
# _data['R1M_Usd_C'] = _data.groupby('date')['R1M_Usd'].transform(lambda x: (x > x.median()))
# _data['R12M_Usd_C'] = _data.groupby('date')['R12M_Usd'].transform(lambda x: (x > x.median()))
# This creates dummy variables indicated whether the return of a given stock was higher than the median cross-section return. This will be used as the Y variable in categorical prediction problems afterwards.. i.e. we'll try to predict which stocks will perform relatively better.
```

```{python}

# fpr, tpr, thresholds = roc_curve()

# Module for AUC computation
```
